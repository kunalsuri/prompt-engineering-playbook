{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00172fc0",
   "metadata": {},
   "source": [
    "# Lab 6 — Agentic Plan-and-Execute\n",
    "\n",
    "**Module reference:** [Module 6, §6.2](https://github.com/kunalsuri/prompt-engineering-playbook/blob/main/learn/06-agentic-patterns.md) — Plan-and-Execute Architecture\n",
    "\n",
    "This lab implements a minimal **plan-and-execute agent** — a foundational agentic architecture — \n",
    "in pure Python (no agent framework required). You'll compare it against a single-prompt baseline \n",
    "to see exactly what decomposition buys you.\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "User Task\n",
    "   │\n",
    "   ▼\n",
    "Planner  →  [Step 1, Step 2, ... Step N]\n",
    "   │\n",
    "   ▼\n",
    "Executor (1 LLM call per step)\n",
    "   │\n",
    "   ▼\n",
    "Synthesizer  →  Final Answer\n",
    "```\n",
    "\n",
    "**What you'll measure:** response quality, reasoning depth, and token cost vs. single-shot baseline.\n",
    "\n",
    "---\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kunalsuri/prompt-engineering-playbook/blob/main/learn/labs/lab_06_agentic_plan_execute.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a43ccc",
   "metadata": {},
   "source": [
    "## Setup — Install dependencies & set API key\n",
    "\n",
    "**Recommended free provider:**\n",
    "- [Google Gemini](https://aistudio.google.com/apikey) — 15 RPM, 1 M tokens/day ⭐\n",
    "- [Groq](https://console.groq.com) — 30 RPM free tier\n",
    "\n",
    "> **Cost note:** The plan-and-execute agent makes N+2 LLM calls per task (planner + N executors + synthesizer).\n",
    "> With a free-tier provider this is fine; with commercial APIs budget ~10× a single-prompt call.\n",
    "> Start with short tasks (2-3 step plans) on your first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ea75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q openai python-dotenv\n",
    "\n",
    "import os\n",
    "# os.environ['GOOGLE_API_KEY'] = 'your-key-here'\n",
    "# os.environ['GROQ_API_KEY']   = 'your-key-here'\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
    "\n",
    "key = os.getenv('GOOGLE_API_KEY') or os.getenv('GROQ_API_KEY') or os.getenv('OPENAI_API_KEY')\n",
    "print(f'✓ API key found ({len(key)} chars)' if key else '⚠ No API key found. Set one above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a4236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('lab_utils.py'):\n",
    "    base = 'https://raw.githubusercontent.com/kunalsuri/prompt-engineering-playbook/main/learn/labs/'\n",
    "    !wget -q {base}lab_utils.py\n",
    "    !wget -q {base}lab_06_agentic_plan_execute.py\n",
    "    !wget -q {base}requirements.txt\n",
    "    %pip install -q -r requirements.txt\n",
    "    print('Lab files downloaded.')\n",
    "else:\n",
    "    print('Lab files already present.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442eae7",
   "metadata": {},
   "source": [
    "## Run the Full Experiment\n",
    "\n",
    "Runs 3 benchmark tasks through both the plan-and-execute agent and the single-prompt baseline, then prints a comparison table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_06_agentic_plan_execute import run_experiment\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9bde73",
   "metadata": {},
   "source": [
    "## Step-by-Step Walkthrough\n",
    "\n",
    "Run just one task to see the full agent trace: planner output, each executor step, and final synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c1ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_06_agentic_plan_execute import run_agent, TASKS\n",
    "\n",
    "task = TASKS[0]  # Change index to try different tasks\n",
    "print(f'Task: {task}\\n{\"=\"*60}')\n",
    "result = run_agent(task)\n",
    "print('\\n--- FINAL ANSWER ---')\n",
    "print(result['answer'])\n",
    "print(f'\\nAPI calls made: {result[\"calls\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8ac496",
   "metadata": {},
   "source": [
    "## Try Your Own Task\n",
    "\n",
    "The agent works best on tasks with 3–6 natural sub-steps (research questions, writing tasks, analysis).\n",
    "Open-ended single questions are served equally well by the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_06_agentic_plan_execute import run_agent, run_single_prompt_baseline\n",
    "\n",
    "my_task = (\n",
    "    \"Compare transformer and LSTM architectures for NLP, \"\n",
    "    \"explain their key trade-offs, and recommend which to use for \"\n",
    "    \"a small-data text classification problem.\"\n",
    ")\n",
    "\n",
    "print('=== PLAN-AND-EXECUTE AGENT ===')\n",
    "agent_result   = run_agent(my_task)\n",
    "print(agent_result['answer'])\n",
    "\n",
    "print('\\n=== SINGLE-PROMPT BASELINE ===')\n",
    "baseline_result = run_single_prompt_baseline(my_task)\n",
    "print(baseline_result['answer'])\n",
    "\n",
    "print(f'\\nAgent calls: {agent_result[\"calls\"]}  |  Baseline calls: {baseline_result[\"calls\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5e5e5",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "1. On which tasks did the agent outperform the baseline? What property made those tasks suited to decomposition?\n",
    "2. How many API calls does the agent make for a 4-step plan? What is the cost multiplier vs. the baseline?\n",
    "3. The planner prompt returns numbered steps in natural language. What could go wrong with that parsing, and how would you harden it?\n",
    "4. The synthesizer receives all step results as a single context window. What happens when a task requires many steps and the context overflows?\n",
    "5. How would you add a **reflection step** where the agent checks its own plan before executing? (Hint: see Module 6 §6.3.)\n",
    "\n",
    "**See also:** [Module 6](https://github.com/kunalsuri/prompt-engineering-playbook/blob/main/learn/06-agentic-patterns.md) §6.2–§6.4 and the [failure-gallery](../failure-gallery/) for real-world breakdowns to watch for."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
