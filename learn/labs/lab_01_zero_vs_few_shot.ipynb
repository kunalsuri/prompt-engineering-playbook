{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31fa4f09",
   "metadata": {},
   "source": [
    "# Lab 1 ‚Äî Zero-Shot vs. Few-Shot Classification\n",
    "\n",
    "**Module reference:** [Module 3, ¬ß3.2‚Äì¬ß3.3](https://github.com/kunalsuri/prompt-engineering-playbook/blob/main/learn/03-patterns.md)\n",
    "\n",
    "This lab sends the same sentiment-classification task to an LLM twice:\n",
    "1. **Zero-shot** ‚Äî instruction only, no examples\n",
    "2. **Few-shot** ‚Äî instruction + 3 demonstration examples\n",
    "\n",
    "It runs each variant multiple times and compares accuracy and consistency.\n",
    "\n",
    "---\n",
    "\n",
    "### Free API Options\n",
    "| Provider | Free Tier | Sign Up |\n",
    "|---|---|---|\n",
    "| **Google Gemini** (recommended) | 15 RPM, 1M tokens/day | [aistudio.google.com/apikey](https://aistudio.google.com/apikey) |\n",
    "| **Groq** | 30 RPM, 14.4K tokens/min | [console.groq.com](https://console.groq.com) |\n",
    "| **OpenAI** (paid) | Pay-per-token | [platform.openai.com](https://platform.openai.com/api-keys) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60689c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üîß Setup ‚Äî Run this cell first\n",
    "!pip install -q openai\n",
    "\n",
    "import getpass, os\n",
    "\n",
    "print(\"Choose your LLM provider (all work with this lab):\")\n",
    "print(\"  1. Google Gemini (FREE ‚Äî recommended)\")\n",
    "print(\"  2. Groq (FREE)\")\n",
    "print(\"  3. OpenAI (paid)\")\n",
    "choice = input(\"\\nEnter 1, 2, or 3: \").strip()\n",
    "\n",
    "if choice == \"1\":\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")\n",
    "    BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "    MODEL = \"gemini-2.0-flash\"\n",
    "elif choice == \"2\":\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n",
    "    BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "    API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "    MODEL = \"llama-3.1-8b-instant\"\n",
    "else:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    BASE_URL = None\n",
    "    API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "    MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "from openai import OpenAI\n",
    "client_kwargs = {\"api_key\": API_KEY}\n",
    "if BASE_URL:\n",
    "    client_kwargs[\"base_url\"] = BASE_URL\n",
    "client = OpenAI(**client_kwargs)\n",
    "\n",
    "def complete(prompt, *, system=\"\", temperature=0.2, max_tokens=1024):\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL, messages=messages,\n",
    "        temperature=temperature, max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content or \"\"\n",
    "\n",
    "print(f\"\\n‚úÖ Connected to {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8151f2",
   "metadata": {},
   "source": [
    "## Test Data & Prompt Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b52aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data: (review_text, expected_label)\n",
    "TEST_CASES = [\n",
    "    (\"The battery lasts all day and the camera is incredible!\", \"Positive\"),\n",
    "    (\"Delivery was fast but the product broke after two days.\", \"Negative\"),\n",
    "    (\"It's okay for the price. Nothing special, nothing terrible.\", \"Neutral\"),\n",
    "    (\"Absolutely love it ‚Äî best purchase I've made this year.\", \"Positive\"),\n",
    "    (\"Waste of money. Returned it immediately.\", \"Negative\"),\n",
    "]\n",
    "\n",
    "SYSTEM = \"You are a sentiment classifier. Respond with exactly one word: Positive, Negative, or Neutral.\"\n",
    "\n",
    "ZERO_SHOT_TEMPLATE = (\n",
    "    \"Classify the sentiment of the following product review.\\n\"\n",
    "    \"Respond with exactly one word: Positive, Negative, or Neutral.\\n\\n\"\n",
    "    \"Review: \\\"{review}\\\"\\n\"\n",
    "    \"Sentiment:\"\n",
    ")\n",
    "\n",
    "FEW_SHOT_TEMPLATE = (\n",
    "    \"Classify the sentiment of product reviews.\\n\"\n",
    "    \"Respond with exactly one word: Positive, Negative, or Neutral.\\n\\n\"\n",
    "    \"Review: \\\"Superb quality and fast shipping!\\\" ‚Üí Positive\\n\"\n",
    "    \"Review: \\\"Stopped working after a week. Very disappointed.\\\" ‚Üí Negative\\n\"\n",
    "    \"Review: \\\"It does what it says. Average product.\\\" ‚Üí Neutral\\n\\n\"\n",
    "    \"Review: \\\"{review}\\\"\\n\"\n",
    "    \"Sentiment:\"\n",
    ")\n",
    "\n",
    "RUNS_PER_VARIANT = 3\n",
    "\n",
    "print(\"Test data and prompts loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81d62d",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a8164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(raw):\n",
    "    cleaned = raw.strip().strip(\".\").strip()\n",
    "    for label in (\"Positive\", \"Negative\", \"Neutral\"):\n",
    "        if label.lower() in cleaned.lower():\n",
    "            return label\n",
    "    return cleaned[:20]\n",
    "\n",
    "\n",
    "results = {\"zero_shot\": [], \"few_shot\": []}\n",
    "\n",
    "for variant_name, template in [(\"zero_shot\", ZERO_SHOT_TEMPLATE), (\"few_shot\", FEW_SHOT_TEMPLATE)]:\n",
    "    print(f\"Running {variant_name.replace('_', '-')} variant ({RUNS_PER_VARIANT} runs √ó {len(TEST_CASES)} cases)...\")\n",
    "    for review, expected in TEST_CASES:\n",
    "        prompt = template.format(review=review)\n",
    "        run_results = []\n",
    "        for _ in range(RUNS_PER_VARIANT):\n",
    "            raw = complete(prompt, system=SYSTEM, temperature=0.3)\n",
    "            predicted = normalize_label(raw)\n",
    "            run_results.append(predicted)\n",
    "\n",
    "        majority = max(set(run_results), key=run_results.count)\n",
    "        correct = majority == expected\n",
    "        consistent = len(set(run_results)) == 1\n",
    "\n",
    "        results[variant_name].append({\n",
    "            \"review\": review[:40] + \"...\",\n",
    "            \"expected\": expected,\n",
    "            \"predictions\": run_results,\n",
    "            \"majority\": majority,\n",
    "            \"correct\": correct,\n",
    "            \"consistent\": consistent,\n",
    "        })\n",
    "\n",
    "print(\"\\n‚úÖ Experiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad043c",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0131055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for variant_name in (\"zero_shot\", \"few_shot\"):\n",
    "    label = variant_name.replace(\"_\", \"-\").title()\n",
    "    print(f\"\\n--- {label} Results ---\")\n",
    "    rows = []\n",
    "    for r in results[variant_name]:\n",
    "        rows.append({\n",
    "            \"Review\": r[\"review\"],\n",
    "            \"Expected\": r[\"expected\"],\n",
    "            \"Predictions\": \", \".join(r[\"predictions\"]),\n",
    "            \"Correct\": \"‚úì\" if r[\"correct\"] else \"‚úó\",\n",
    "            \"Consistent\": \"‚úì\" if r[\"consistent\"] else \"‚úó\",\n",
    "        })\n",
    "    display(pd.DataFrame(rows))\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"  SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "zs = results[\"zero_shot\"]\n",
    "fs = results[\"few_shot\"]\n",
    "zs_acc = sum(1 for r in zs if r[\"correct\"]) / len(zs) * 100\n",
    "fs_acc = sum(1 for r in fs if r[\"correct\"]) / len(fs) * 100\n",
    "zs_con = sum(1 for r in zs if r[\"consistent\"]) / len(zs) * 100\n",
    "fs_con = sum(1 for r in fs if r[\"consistent\"]) / len(fs) * 100\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\"Metric\": \"Accuracy (majority vote)\", \"Zero-Shot\": f\"{zs_acc:.0f}%\", \"Few-Shot\": f\"{fs_acc:.0f}%\"},\n",
    "    {\"Metric\": \"Consistency (all runs agree)\", \"Zero-Shot\": f\"{zs_con:.0f}%\", \"Few-Shot\": f\"{fs_con:.0f}%\"},\n",
    "])\n",
    "display(summary)\n",
    "\n",
    "print(\"\\nüìù Takeaway: Few-shot examples typically improve both accuracy and consistency\")\n",
    "print(\"on classification tasks, especially for ambiguous or edge-case inputs.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
