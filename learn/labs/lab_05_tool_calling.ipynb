{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ab7258",
   "metadata": {},
   "source": [
    "# Lab 5 — Tool-Calling & Structured Output\n",
    "\n",
    "**Module reference:** [Module 3, §3.6](https://github.com/kunalsuri/prompt-engineering-playbook/blob/main/learn/03-patterns.md) — Constrained Output  \n",
    "[Module 5, §5.4](https://github.com/kunalsuri/prompt-engineering-playbook/blob/main/learn/05-advanced-patterns.md) — Evaluation Pipelines\n",
    "\n",
    "This lab compares two strategies for getting reliably structured JSON output from LLMs:\n",
    "\n",
    "1. **JSON-Mode Prompting** — natural-language instructions to emit JSON; validate with `json.loads()`\n",
    "2. **Function-Calling (Tool-Calling)** — typed function schema enforced at the API level\n",
    "\n",
    "**What you'll measure:** valid-JSON rate, field completeness, and consistency across runs.\n",
    "\n",
    "---\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kunalsuri/prompt-engineering-playbook/blob/main/learn/labs/lab_05_tool_calling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79041504",
   "metadata": {},
   "source": [
    "## Setup — Install dependencies & set API key\n",
    "\n",
    "**Free providers (no credit card required):**\n",
    "- [Google Gemini](https://aistudio.google.com/apikey) — 15 RPM, 1M tokens/day ⭐\n",
    "- [Groq](https://console.groq.com) — 30 RPM free tier\n",
    "\n",
    "> **Note on tool-calling:** For the function-calling strategy, OpenAI or Google Gemini are recommended.\n",
    "> Groq supports tool-calling on select models (llama3, mixtral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (first run only)\n",
    "%pip install -q openai python-dotenv\n",
    "\n",
    "import os\n",
    "# Uncomment ONE of the following and paste your API key:\n",
    "# os.environ['GOOGLE_API_KEY'] = 'your-key-here'   # Free at aistudio.google.com/apikey\n",
    "# os.environ['GROQ_API_KEY']   = 'your-key-here'   # Free at console.groq.com\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
    "\n",
    "key = os.getenv('GOOGLE_API_KEY') or os.getenv('GROQ_API_KEY') or os.getenv('OPENAI_API_KEY')\n",
    "print(f'✓ API key found ({len(key)} chars)' if key else '⚠ No API key found. Set one above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download lab files if running in Colab\n",
    "import os\n",
    "if not os.path.exists('lab_utils.py'):\n",
    "    base = 'https://raw.githubusercontent.com/kunalsuri/prompt-engineering-playbook/main/learn/labs/'\n",
    "    !wget -q {base}lab_utils.py\n",
    "    !wget -q {base}lab_05_tool_calling.py\n",
    "    !wget -q {base}requirements.txt\n",
    "    %pip install -q -r requirements.txt\n",
    "    print('Lab files downloaded.')\n",
    "else:\n",
    "    print('Lab files already present.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb5389",
   "metadata": {},
   "source": [
    "## Run the Experiment\n",
    "\n",
    "The experiment tests both strategies on 5 unstructured product descriptions.\n",
    "Results appear in a comparison table showing valid-JSON rate and field completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9170f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_05_tool_calling import run_experiment\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa55654",
   "metadata": {},
   "source": [
    "## Explore Individual Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae4e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from lab_utils import get_client\n",
    "from lab_05_tool_calling import run_json_mode, run_tool_calling, PRODUCT_DESCRIPTIONS\n",
    "\n",
    "client = get_client()\n",
    "description = PRODUCT_DESCRIPTIONS[0]\n",
    "\n",
    "print('Testing JSON-mode on:', description[:60], '...\\n')\n",
    "results = run_json_mode(description, client)\n",
    "for i, r in enumerate(results, 1):\n",
    "    if r['success']:\n",
    "        print(f'Run {i}: ✓  →  {json.dumps(r[\"data\"], indent=2)}')\n",
    "    else:\n",
    "        print(f'Run {i}: ✗  →  Invalid JSON: {r[\"error\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d446552",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "1. Which strategy had a higher valid-JSON rate on your run? Was the difference significant?\n",
    "2. Did the model wrap output in markdown fences despite instructions? How did the JSON-mode code handle that?\n",
    "3. Under what conditions would you choose JSON-mode over tool-calling in a production system?\n",
    "4. How does constraining `key_specs` to be an `array` change model behavior compared to a plain `string` field?\n",
    "\n",
    "**See also:** Module 3 §3.6 (Constrained Output) and [failure-gallery case 04](../failure-gallery/04-ambiguous-format/) for the failure this lab fixes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
