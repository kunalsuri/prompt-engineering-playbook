{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b2fe27",
   "metadata": {},
   "source": [
    "# Lab 4 ‚Äî Building a Mini Evaluation Pipeline\n",
    "\n",
    "**Module reference:** [Module 5, ¬ß5.4](https://github.com/kunalsuri/prompt-engineering-playbook/blob/main/learn/05-advanced-patterns.md)\n",
    "\n",
    "This lab builds a complete prompt evaluation pipeline:\n",
    "1. Define a test suite of (input, criteria) pairs\n",
    "2. Run **two prompt variants** against the suite\n",
    "3. Score outputs using **heuristics** and **LLM-as-Judge**\n",
    "4. Aggregate metrics and declare a winner\n",
    "\n",
    "This mirrors the methodology in the Advanced Patterns module and the `evaluation-template.md` shared resource.\n",
    "\n",
    "---\n",
    "\n",
    "### Free API Options\n",
    "| Provider | Free Tier | Sign Up |\n",
    "|---|---|---|\n",
    "| **Google Gemini** (recommended) | 15 RPM, 1M tokens/day | [aistudio.google.com/apikey](https://aistudio.google.com/apikey) |\n",
    "| **Groq** | 30 RPM, 14.4K tokens/min | [console.groq.com](https://console.groq.com) |\n",
    "| **OpenAI** (paid) | Pay-per-token | [platform.openai.com](https://platform.openai.com/api-keys) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üîß Setup ‚Äî Run this cell first\n",
    "!pip install -q openai pandas\n",
    "\n",
    "import getpass, os, json, time\n",
    "\n",
    "print(\"Choose your LLM provider (all work with this lab):\")\n",
    "print(\"  1. Google Gemini (FREE ‚Äî recommended)\")\n",
    "print(\"  2. Groq (FREE)\")\n",
    "print(\"  3. OpenAI (paid)\")\n",
    "choice = input(\"\\nEnter 1, 2, or 3: \").strip()\n",
    "\n",
    "if choice == \"1\":\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")\n",
    "    BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "    MODEL = \"gemini-2.0-flash\"\n",
    "elif choice == \"2\":\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n",
    "    BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "    API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "    MODEL = \"llama-3.1-8b-instant\"\n",
    "else:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    BASE_URL = None\n",
    "    API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "    MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "from openai import OpenAI\n",
    "client_kwargs = {\"api_key\": API_KEY}\n",
    "if BASE_URL:\n",
    "    client_kwargs[\"base_url\"] = BASE_URL\n",
    "client = OpenAI(**client_kwargs)\n",
    "\n",
    "def complete(prompt, *, system=\"\", temperature=0.3, max_tokens=1024):\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL, messages=messages,\n",
    "        temperature=temperature, max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content or \"\"\n",
    "\n",
    "print(f\"\\n‚úÖ Connected to {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5240e74",
   "metadata": {},
   "source": [
    "## Test Suite & Prompt Variants\n",
    "\n",
    "We test **email subject-line generation** with 5 diverse email bodies. Two prompt variants are compared:\n",
    "- **Variant A** ‚Äî basic, minimal instruction\n",
    "- **Variant B** ‚Äî constrained (length, verb usage, no spam words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c78050",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SUITE = [\n",
    "    {\n",
    "        \"input\": \"We're launching a 30% off sale on all running shoes this weekend.\",\n",
    "        \"criteria\": \"mentions discount percentage and product category\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Reminder: your annual subscription renews in 3 days. Update payment info if needed.\",\n",
    "        \"criteria\": \"conveys urgency and mentions renewal timeline\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Thank you for attending our webinar on cloud security. Here are the slides and recording.\",\n",
    "        \"criteria\": \"references the webinar topic and mentions deliverables\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"We've updated our privacy policy effective January 1. Please review the changes.\",\n",
    "        \"criteria\": \"mentions policy update and effective date\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Congratulations! You've been selected for early access to our new AI assistant.\",\n",
    "        \"criteria\": \"conveys exclusivity and names the product\",\n",
    "    },\n",
    "]\n",
    "\n",
    "SYSTEM = \"You are an email marketing specialist.\"\n",
    "\n",
    "VARIANT_A = (\n",
    "    \"Write a subject line for this email:\\n\\n\"\n",
    "    \"{body}\\n\\n\"\n",
    "    \"Subject line:\"\n",
    ")\n",
    "\n",
    "VARIANT_B = (\n",
    "    \"Write a compelling email subject line that is:\\n\"\n",
    "    \"- Under 60 characters\\n\"\n",
    "    \"- Action-oriented (uses a verb)\\n\"\n",
    "    \"- Specific about the key benefit or information\\n\"\n",
    "    \"- Free of spam trigger words (FREE, URGENT, !!!)\\n\\n\"\n",
    "    \"Email body:\\n{body}\\n\\n\"\n",
    "    \"Subject line:\"\n",
    ")\n",
    "\n",
    "print(f\"Test suite: {len(TEST_SUITE)} cases\")\n",
    "print(f\"Variant A: basic prompt ({len(VARIANT_A)} chars)\")\n",
    "print(f\"Variant B: constrained prompt ({len(VARIANT_B)} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54887842",
   "metadata": {},
   "source": [
    "## Step 1 ‚Äî Generate Subject Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55123ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "variant_outputs = {\"A\": [], \"B\": []}\n",
    "\n",
    "for i, case in enumerate(TEST_SUITE):\n",
    "    for variant_name, template in [(\"A\", VARIANT_A), (\"B\", VARIANT_B)]:\n",
    "        prompt = template.format(body=case[\"input\"])\n",
    "        raw = complete(prompt, system=SYSTEM, temperature=0.3)\n",
    "        subject = raw.strip().strip('\"').strip(\"'\").splitlines()[0]\n",
    "        variant_outputs[variant_name].append(subject)\n",
    "    print(f\"  Case {i+1}/5 done\")\n",
    "    time.sleep(0.3)  # gentle rate limiting\n",
    "\n",
    "df_gen = pd.DataFrame({\n",
    "    \"Email (truncated)\": [c[\"input\"][:45] + \"...\" for c in TEST_SUITE],\n",
    "    \"Variant A\": [s[:55] for s in variant_outputs[\"A\"]],\n",
    "    \"Variant B\": [s[:55] for s in variant_outputs[\"B\"]],\n",
    "})\n",
    "display(df_gen)\n",
    "print(\"\\n‚úÖ All subject lines generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7374d91",
   "metadata": {},
   "source": [
    "## Step 2 ‚Äî Heuristic Checks (Length ‚â§ 60 chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ef1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(len(TEST_SUITE)):\n",
    "    a_subj = variant_outputs[\"A\"][i]\n",
    "    b_subj = variant_outputs[\"B\"][i]\n",
    "    rows.append({\n",
    "        \"Case\": i + 1,\n",
    "        \"A Length\": len(a_subj),\n",
    "        \"A Pass\": \"‚úì\" if len(a_subj) <= 60 else \"‚úó\",\n",
    "        \"B Length\": len(b_subj),\n",
    "        \"B Pass\": \"‚úì\" if len(b_subj) <= 60 else \"‚úó\",\n",
    "    })\n",
    "\n",
    "df_len = pd.DataFrame(rows)\n",
    "display(df_len)\n",
    "\n",
    "a_pass = sum(1 for s in variant_outputs[\"A\"] if len(s) <= 60)\n",
    "b_pass = sum(1 for s in variant_outputs[\"B\"] if len(s) <= 60)\n",
    "print(f\"\\nLength compliance: Variant A = {a_pass}/{len(TEST_SUITE)}, Variant B = {b_pass}/{len(TEST_SUITE)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab397742",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äî LLM-as-Judge Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df002678",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_SYSTEM = \"You are a strict email marketing evaluator. Return ONLY valid JSON.\"\n",
    "\n",
    "JUDGE_TEMPLATE = (\n",
    "    \"Evaluate this email subject line on each criterion (1-5 scale).\\n\\n\"\n",
    "    \"Email body: {body}\\n\"\n",
    "    \"Subject line: {subject}\\n\"\n",
    "    \"Quality criteria: {criteria}\\n\\n\"\n",
    "    \"Score these dimensions and return ONLY a JSON object:\\n\"\n",
    "    \"- relevance: Does it accurately reflect the email content? (1-5)\\n\"\n",
    "    \"- clarity: Is it clear and easy to understand? (1-5)\\n\"\n",
    "    \"- engagement: Would it entice the reader to open the email? (1-5)\\n\"\n",
    "    \"- criteria_met: Does it meet the specific criteria above? (1-5)\\n\"\n",
    "    \"- conciseness: Is it appropriately brief? (1-5)\\n\\n\"\n",
    "    \"JSON:\"\n",
    ")\n",
    "\n",
    "dimensions = [\"relevance\", \"clarity\", \"engagement\", \"criteria_met\", \"conciseness\"]\n",
    "all_scores = {\"A\": [], \"B\": []}\n",
    "\n",
    "for i, case in enumerate(TEST_SUITE):\n",
    "    for variant_name in (\"A\", \"B\"):\n",
    "        subject = variant_outputs[variant_name][i]\n",
    "        prompt = JUDGE_TEMPLATE.format(\n",
    "            body=case[\"input\"], subject=subject, criteria=case[\"criteria\"]\n",
    "        )\n",
    "        raw = complete(prompt, system=JUDGE_SYSTEM, temperature=0.0)\n",
    "        try:\n",
    "            start = raw.find(\"{\")\n",
    "            end = raw.rfind(\"}\") + 1\n",
    "            scores = json.loads(raw[start:end]) if start >= 0 else {}\n",
    "        except (json.JSONDecodeError, ValueError):\n",
    "            scores = {}\n",
    "        all_scores[variant_name].append(scores)\n",
    "        time.sleep(0.3)\n",
    "    print(f\"  Case {i+1}/5 judged\")\n",
    "\n",
    "# Per-case detail\n",
    "for i, case in enumerate(TEST_SUITE):\n",
    "    print(f\"\\n  Email {i+1}: {case['input'][:50]}...\")\n",
    "    for v in (\"A\", \"B\"):\n",
    "        s = all_scores[v][i]\n",
    "        score_str = \", \".join(f\"{d}={s.get(d, '?')}\" for d in dimensions)\n",
    "        print(f\"    Variant {v}: {score_str}\")\n",
    "\n",
    "print(\"\\n‚úÖ Judging complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab91145d",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äî Aggregate Results & Declare Winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cc9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_score(variant, dim):\n",
    "    vals = [s.get(dim, 0) for s in all_scores[variant] if isinstance(s.get(dim), (int, float))]\n",
    "    return sum(vals) / len(vals) if vals else 0.0\n",
    "\n",
    "rows = []\n",
    "a_wins, b_wins = 0, 0\n",
    "for dim in dimensions:\n",
    "    a_avg = avg_score(\"A\", dim)\n",
    "    b_avg = avg_score(\"B\", dim)\n",
    "    if a_avg > b_avg:\n",
    "        winner = \"A\"; a_wins += 1\n",
    "    elif b_avg > a_avg:\n",
    "        winner = \"B\"; b_wins += 1\n",
    "    else:\n",
    "        winner = \"Tie\"\n",
    "    rows.append({\"Dimension\": dim.replace('_', ' ').title(), \"Variant A\": f\"{a_avg:.1f}\", \"Variant B\": f\"{b_avg:.1f}\", \"Winner\": winner})\n",
    "\n",
    "# Add length compliance row\n",
    "rows.append({\"Dimension\": \"Length ‚â§60\", \"Variant A\": f\"{a_pass}/{len(TEST_SUITE)}\", \"Variant B\": f\"{b_pass}/{len(TEST_SUITE)}\",\n",
    "             \"Winner\": \"A\" if a_pass > b_pass else (\"B\" if b_pass > a_pass else \"Tie\")})\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "display(df_results)\n",
    "\n",
    "overall = \"Variant B\" if b_wins > a_wins else (\"Variant A\" if a_wins > b_wins else \"Tie\")\n",
    "print(f\"\\nüèÜ Variant A won {a_wins} dimensions, Variant B won {b_wins} dimensions.\")\n",
    "print(f\"   Overall winner: {overall}\")\n",
    "print()\n",
    "print(\"üìù Takeaway: Variant B's explicit constraints (length, verb usage, no spam words)\")\n",
    "print(\"give the model concrete success criteria, producing more consistent, higher-quality\")\n",
    "print(\"output. This demonstrates the value of building evaluation pipelines to objectively\")\n",
    "print(\"compare prompt variants.\")\n",
    "print(\"See Module 5, ¬ß5.4 and prompts/shared/evaluation-template.md for methodology.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
