{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5fd5206",
   "metadata": {},
   "source": [
    "# Lab 3 â€” The Specificity Spectrum\n",
    "\n",
    "**Module reference:** [Module 2, Â§2.1](https://github.com/kunalsuri/prompt-engineering-playbook/blob/main/learn/02-core-principles.md)\n",
    "\n",
    "This lab demonstrates how increasing prompt specificity improves output quality. It sends three versions of the same request â€” **vague**, **moderate**, and **highly specific** â€” then scores the outputs using an **LLM-as-Judge** evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### Free API Options\n",
    "| Provider | Free Tier | Sign Up |\n",
    "|---|---|---|\n",
    "| **Google Gemini** (recommended) | 15 RPM, 1M tokens/day | [aistudio.google.com/apikey](https://aistudio.google.com/apikey) |\n",
    "| **Groq** | 30 RPM, 14.4K tokens/min | [console.groq.com](https://console.groq.com) |\n",
    "| **OpenAI** (paid) | Pay-per-token | [platform.openai.com](https://platform.openai.com/api-keys) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91184233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ”§ Setup â€” Run this cell first\n",
    "!pip install -q openai\n",
    "\n",
    "import getpass, os\n",
    "\n",
    "print(\"Choose your LLM provider (all work with this lab):\")\n",
    "print(\"  1. Google Gemini (FREE â€” recommended)\")\n",
    "print(\"  2. Groq (FREE)\")\n",
    "print(\"  3. OpenAI (paid)\")\n",
    "choice = input(\"\\nEnter 1, 2, or 3: \").strip()\n",
    "\n",
    "if choice == \"1\":\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")\n",
    "    BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "    MODEL = \"gemini-2.0-flash\"\n",
    "elif choice == \"2\":\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n",
    "    BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "    API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "    MODEL = \"llama-3.1-8b-instant\"\n",
    "else:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    BASE_URL = None\n",
    "    API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "    MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "from openai import OpenAI\n",
    "client_kwargs = {\"api_key\": API_KEY}\n",
    "if BASE_URL:\n",
    "    client_kwargs[\"base_url\"] = BASE_URL\n",
    "client = OpenAI(**client_kwargs)\n",
    "\n",
    "def complete(prompt, *, system=\"\", temperature=0.3, max_tokens=1024):\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL, messages=messages,\n",
    "        temperature=temperature, max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content or \"\"\n",
    "\n",
    "print(f\"\\nâœ… Connected to {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a1ba30",
   "metadata": {},
   "source": [
    "## Three Prompt Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = \"You are a helpful writing assistant.\"\n",
    "\n",
    "PROMPTS = {\n",
    "    \"vague\": \"Write something about Python testing.\",\n",
    "    \"moderate\": (\n",
    "        \"Write a guide about testing in Python. Cover unit tests and \"\n",
    "        \"integration tests. Include code examples.\"\n",
    "    ),\n",
    "    \"specific\": (\n",
    "        \"Write a concise guide (300-400 words) on Python testing best \"\n",
    "        \"practices for a mid-level developer. Structure the guide with \"\n",
    "        \"these sections:\\n\"\n",
    "        \"1. **Unit Tests** â€” explain the AAA pattern (Arrange, Act, Assert) \"\n",
    "        \"with a pytest example testing a `calculate_discount(price, percent)` \"\n",
    "        \"function.\\n\"\n",
    "        \"2. **Integration Tests** â€” show a pytest example that tests a \"\n",
    "        \"FastAPI endpoint using `TestClient`.\\n\"\n",
    "        \"3. **Key Principles** â€” list 3 bullet-point testing principles \"\n",
    "        \"(e.g., test isolation, determinism, meaningful names).\\n\\n\"\n",
    "        \"Use fenced Python code blocks. End with a one-sentence summary.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"Prompts defined at three specificity levels.\")\n",
    "for level, prompt in PROMPTS.items():\n",
    "    print(f\"  {level:>10}: {prompt[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebfa90d",
   "metadata": {},
   "source": [
    "## Generate Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eced0729",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "for level, prompt in PROMPTS.items():\n",
    "    print(f\"Generating {level} output...\")\n",
    "    outputs[level] = complete(prompt, system=SYSTEM)\n",
    "    print(f\"  â†’ {len(outputs[level])} chars, {len(outputs[level].split())} words\\n\")\n",
    "\n",
    "print(\"âœ… All outputs generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aead2588",
   "metadata": {},
   "source": [
    "## LLM-as-Judge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35388436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "EVAL_SYSTEM = \"You are a strict technical writing evaluator. Return ONLY valid JSON.\"\n",
    "\n",
    "EVAL_TEMPLATE = (\n",
    "    \"Evaluate the following technical guide on a 1-5 scale for each criterion.\\n\"\n",
    "    \"Return ONLY a JSON object with these keys:\\n\"\n",
    "    \"- structure: Does it have clear sections/headers? (1=none, 5=excellent)\\n\"\n",
    "    \"- detail: Are explanations specific and actionable? (1=vague, 5=very specific)\\n\"\n",
    "    \"- code_quality: Are code examples present, correct, and useful? (1=none/poor, 5=excellent)\\n\"\n",
    "    \"- completeness: Does it cover the topic adequately? (1=minimal, 5=comprehensive)\\n\"\n",
    "    \"- overall: Overall quality (1=poor, 5=excellent)\\n\\n\"\n",
    "    \"Guide to evaluate:\\n---\\n{guide}\\n---\\n\\nJSON:\"\n",
    ")\n",
    "\n",
    "scores = {}\n",
    "for level, output in outputs.items():\n",
    "    print(f\"Evaluating {level} output...\")\n",
    "    raw = complete(EVAL_TEMPLATE.format(guide=output[:2000]), system=EVAL_SYSTEM, temperature=0.0)\n",
    "    try:\n",
    "        start = raw.find(\"{\")\n",
    "        end = raw.rfind(\"}\") + 1\n",
    "        scores[level] = json.loads(raw[start:end]) if start >= 0 else {}\n",
    "    except (json.JSONDecodeError, ValueError):\n",
    "        scores[level] = {}\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032cf2e4",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb44c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "criteria = [\"structure\", \"detail\", \"code_quality\", \"completeness\", \"overall\"]\n",
    "rows = []\n",
    "for c in criteria:\n",
    "    rows.append({\n",
    "        \"Criterion\": c.replace(\"_\", \" \").title(),\n",
    "        \"Vague\": scores.get(\"vague\", {}).get(c, \"?\"),\n",
    "        \"Moderate\": scores.get(\"moderate\", {}).get(c, \"?\"),\n",
    "        \"Specific\": scores.get(\"specific\", {}).get(c, \"?\"),\n",
    "    })\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nðŸ“ Takeaway: More specific prompts consistently produce higher-quality,\")\n",
    "print(\"more structured output. The 'specific' prompt constrains word count, defines\")\n",
    "print(\"exact sections, names functions, and specifies the audience.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3164b",
   "metadata": {},
   "source": [
    "## Output Previews\n",
    "\n",
    "Expand the cells below to see the actual generated output at each specificity level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac126a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Vague Output (click to expand)\n",
    "print(outputs[\"vague\"][:800])\n",
    "if len(outputs[\"vague\"]) > 800:\n",
    "    print(\"\\n... [truncated]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Moderate Output (click to expand)\n",
    "print(outputs[\"moderate\"][:800])\n",
    "if len(outputs[\"moderate\"]) > 800:\n",
    "    print(\"\\n... [truncated]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98498b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Specific Output (click to expand)\n",
    "print(outputs[\"specific\"][:800])\n",
    "if len(outputs[\"specific\"]) > 800:\n",
    "    print(\"\\n... [truncated]\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
