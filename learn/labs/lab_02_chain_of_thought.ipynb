{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af0b00e",
   "metadata": {},
   "source": [
    "# Lab 2 â€” Chain-of-Thought Prompting\n",
    "\n",
    "**Module reference:** [Module 3, Â§3.4](https://github.com/kunalsuri/prompt-engineering-playbook/blob/main/learn/03-patterns.md)\n",
    "\n",
    "This lab compares **direct-answer** prompting with **Chain-of-Thought (CoT)** on multi-step arithmetic / reasoning problems.\n",
    "\n",
    "**Hypothesis:** CoT reduces errors on problems requiring intermediate steps.\n",
    "\n",
    "---\n",
    "\n",
    "### Free API Options\n",
    "| Provider | Free Tier | Sign Up |\n",
    "|---|---|---|\n",
    "| **Google Gemini** (recommended) | 15 RPM, 1M tokens/day | [aistudio.google.com/apikey](https://aistudio.google.com/apikey) |\n",
    "| **Groq** | 30 RPM, 14.4K tokens/min | [console.groq.com](https://console.groq.com) |\n",
    "| **OpenAI** (paid) | Pay-per-token | [platform.openai.com](https://platform.openai.com/api-keys) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb82dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ”§ Setup â€” Run this cell first\n",
    "!pip install -q openai\n",
    "\n",
    "import getpass, os\n",
    "\n",
    "print(\"Choose your LLM provider (all work with this lab):\")\n",
    "print(\"  1. Google Gemini (FREE â€” recommended)\")\n",
    "print(\"  2. Groq (FREE)\")\n",
    "print(\"  3. OpenAI (paid)\")\n",
    "choice = input(\"\\nEnter 1, 2, or 3: \").strip()\n",
    "\n",
    "if choice == \"1\":\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")\n",
    "    BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "    MODEL = \"gemini-2.0-flash\"\n",
    "elif choice == \"2\":\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n",
    "    BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "    API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "    MODEL = \"llama-3.1-8b-instant\"\n",
    "else:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    BASE_URL = None\n",
    "    API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "    MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "from openai import OpenAI\n",
    "client_kwargs = {\"api_key\": API_KEY}\n",
    "if BASE_URL:\n",
    "    client_kwargs[\"base_url\"] = BASE_URL\n",
    "client = OpenAI(**client_kwargs)\n",
    "\n",
    "def complete(prompt, *, system=\"\", temperature=0.0, max_tokens=1024):\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL, messages=messages,\n",
    "        temperature=temperature, max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content or \"\"\n",
    "\n",
    "print(f\"\\nâœ… Connected to {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa1c97",
   "metadata": {},
   "source": [
    "## Test Problems & Prompt Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239baaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBLEMS = [\n",
    "    (\"A store has 23 apples. They sell 8, then receive a shipment of 15. A customer returns 3 apples. How many apples does the store have?\", \"33\"),\n",
    "    (\"A train travels at 60 km/h for 2.5 hours, then at 80 km/h for 1.5 hours. What is the total distance traveled?\", \"270\"),\n",
    "    (\"Maria has $50. She buys 3 books at $8 each and 2 pens at $3 each. How much money does she have left?\", \"20\"),\n",
    "    (\"A rectangular garden is 12 meters long and 8 meters wide. A path 1 meter wide surrounds the garden. What is the area of the path?\", \"44\"),\n",
    "    (\"In a class of 30 students, 40% prefer math, 30% prefer science, and the rest prefer art. How many students prefer art?\", \"9\"),\n",
    "]\n",
    "\n",
    "SYSTEM = \"You are a helpful math tutor.\"\n",
    "\n",
    "DIRECT_TEMPLATE = (\n",
    "    \"Answer the following question. Provide ONLY the final numerical answer, \"\n",
    "    \"with no units or explanation.\\n\\n\"\n",
    "    \"Question: {question}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "COT_TEMPLATE = (\n",
    "    \"Answer the following question. Think step by step, showing your work. \"\n",
    "    \"After your reasoning, provide the final numerical answer on a new line \"\n",
    "    \"prefixed with 'ANSWER: '.\\n\\n\"\n",
    "    \"Question: {question}\\n\"\n",
    "    \"Solution:\"\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(PROBLEMS)} test problems.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f28786",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd762d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_number(raw):\n",
    "    for line in reversed(raw.strip().splitlines()):\n",
    "        if \"ANSWER:\" in line.upper():\n",
    "            parts = line.upper().split(\"ANSWER:\")\n",
    "            num_str = parts[-1].strip().rstrip(\".\")\n",
    "            tokens = num_str.split()\n",
    "            if tokens:\n",
    "                return tokens[0].replace(\",\", \"\").replace(\"$\", \"\")\n",
    "    numbers = re.findall(r\"-?\\d+\\.?\\d*\", raw)\n",
    "    return numbers[-1] if numbers else raw.strip()[:20]\n",
    "\n",
    "def answers_match(predicted, expected):\n",
    "    try:\n",
    "        return abs(float(predicted) - float(expected)) < 0.5\n",
    "    except ValueError:\n",
    "        return predicted.strip() == expected.strip()\n",
    "\n",
    "\n",
    "results = {\"direct\": [], \"cot\": []}\n",
    "\n",
    "for variant_name, template in [(\"direct\", DIRECT_TEMPLATE), (\"cot\", COT_TEMPLATE)]:\n",
    "    label = \"Direct\" if variant_name == \"direct\" else \"CoT\"\n",
    "    print(f\"Running {label} variant on {len(PROBLEMS)} problems...\")\n",
    "    for question, expected in PROBLEMS:\n",
    "        prompt = template.format(question=question)\n",
    "        raw = complete(prompt, system=SYSTEM, temperature=0.0)\n",
    "        predicted = extract_number(raw)\n",
    "        correct = answers_match(predicted, expected)\n",
    "        results[variant_name].append({\n",
    "            \"question\": question[:50] + \"...\",\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": predicted,\n",
    "            \"correct\": correct,\n",
    "            \"raw_snippet\": raw.strip()[:100].replace(\"\\n\", \" \"),\n",
    "        })\n",
    "\n",
    "print(\"\\nâœ… Experiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b6def6",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for variant_name in (\"direct\", \"cot\"):\n",
    "    label = \"Direct Answer\" if variant_name == \"direct\" else \"Chain-of-Thought\"\n",
    "    print(f\"\\n--- {label} Results ---\")\n",
    "    rows = []\n",
    "    for r in results[variant_name]:\n",
    "        rows.append({\n",
    "            \"Problem\": r[\"question\"],\n",
    "            \"Expected\": r[\"expected\"],\n",
    "            \"Predicted\": r[\"predicted\"],\n",
    "            \"Correct\": \"âœ“\" if r[\"correct\"] else \"âœ—\",\n",
    "        })\n",
    "    display(pd.DataFrame(rows))\n",
    "\n",
    "# Summary\n",
    "d_acc = sum(1 for r in results[\"direct\"] if r[\"correct\"]) / len(PROBLEMS) * 100\n",
    "c_acc = sum(1 for r in results[\"cot\"] if r[\"correct\"]) / len(PROBLEMS) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "summary = pd.DataFrame([{\"Metric\": \"Accuracy\", \"Direct\": f\"{d_acc:.0f}%\", \"CoT\": f\"{c_acc:.0f}%\"}])\n",
    "display(summary)\n",
    "\n",
    "print(\"\\nðŸ“ Takeaway: Chain-of-Thought prompting helps the model 'show its work',\")\n",
    "print(\"reducing arithmetic errors on multi-step problems.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af853ac",
   "metadata": {},
   "source": [
    "## Example CoT Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ca636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a full CoT reasoning trace for Problem 1\n",
    "prompt = COT_TEMPLATE.format(question=PROBLEMS[0][0])\n",
    "trace = complete(prompt, system=SYSTEM, temperature=0.0)\n",
    "print(\"Question:\", PROBLEMS[0][0])\n",
    "print(\"\\n--- CoT Trace ---\")\n",
    "print(trace)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
